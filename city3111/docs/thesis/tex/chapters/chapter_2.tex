% chapter 2
\cleardoublepage
\phantomsection
\chapter{Requirements and Analysis}
\section{My proposed system: Discussion on compute hardware choices}
While developing my system, the first thing I need to consider is the hardware it will be built upon. Specifically, I need to consider the computer system or systems I will use, the network hardware that will be required, any peripherals for those two interconnects and the associated costs involved with acquiring this hardware.

Before I delve into this discussion, I must first lay out some assumptions on any hardware setup I go forwards with in order to ensure there is a good understanding of what is required of my system in order for any derived results to be viewed as fair and scientific.

Firstly, any compute nodes used within my cluster must match any other compute node -- including the master. For example, if a system within the cluster has an i7 CPU and another has an i3 CPU, this cannot be considered a scientific test as the cluster will likely show uneven speedups in the final results.

Lastly, any interconnect in use (i.e. 1 000 BASE-T Ethernet) must be uniform in nature. For example, if one system has a 1 000 BASE-T Ethernet adapter and another has a 100 BASE-T Ethernet adapter, this cannot be viewed as a fair test as nodes within the cluster will be communicating at different baud rates.
\vfill\break

\textbf{\sffamily{Raspberry Pi as cluster}}

Starting now on this discussion, the first consideration is to use a cluster of five Raspberry Pi 4s connected to a Gigabit switch via Cat5e Ethernet. This is inspired in large part by the research completed at the University of Southampton, where Pi Model Bs were assembled in a similar manner \cite{cox_et_al_2013}. By utilising lots of small compute nodes, I would hope to achieve cluster built atop MPI capable of very high flop/watt performance at a reasonable price point.

The decision to build my cluster using five nodes was born out of the thought process that the more nodes I can have, the more data I can collect. Five nodes of quad-core machines would result in twenty NUMA nodes in total -- sufficient processor counts to offer meaningful insight into the performance of my system.

Unfortunately, during my feasibility study, I have found that using Raspberry Pi systems is not possible in my research scope due to budgetary constraints (that is, I have neither an institutional nor a personal budget) and the required hardware is not available for lease at my institution.

\textbf{\sffamily{A compromise: Utilising a DS}}

To complete my cluster, I have ended up needing to utilise hardware existing to me. To this end, I have decided to use my Dell PowerEdge R710 featuring 2 x Intel Xeon CPU X5660 @ 2,80GHz with 96 GB of DDR3 ECC memory. This system also has four 1 000 BASE-T Ethernet ports making it suitable for external management. I will run five guests within my hypervisor in order to simulate a good range of loads on my dedicated server (DS).

Using a production server comes with its own set of challenges. How do you balance the resource requirements of other virtual guests in order to maintain a fair test? How can you ensure all systems communicate fairly (i.e. one system isn't getting priority CPU or network time)? Are there any additional considerations required when designing such a system? Try try and answer those questions, I'd like to express how I intend on mitigating and adapting the load on my server machine while operating the program and cluster.

The first and obvious step I would take is to run my programs at off-peak hours (typically night time as the majority of my users are local to Europe). By doing this, I ensure that step two has minimal business impact on myself.

This leads onto the second alluded to mitigation step, which is to shut down all non-essential guests running on the server. By doing this, only CPU and network time essential to maintaining a connection to both the internet and my local area network are being consumed, leaving the vast majority of time available for my project program.

Other more adaptive tasks can be conducted on the fly as the network environment changes, which might include disabling or turning off some network-connected devices that appear to consume disproportionate network resources. Unfortunately these tactics cannot be planned for but simply reacted to in a timely manner without a more dedicated laboratory setup.

\section{\sffamily{My proposed system: Discussion on software choices}}
As explored in my literature review, Open MPI appears to provide the best available bedrock for producing a cluster computer using commodity hardware as it offers a balance of diverse and stable APIs and good availability for a plethora of system and network architectures. Therefore, the choice with regards to software to use within my system is more aimed at the Operating System (OS) end of the software spectrum.

\textbf{\sffamily{A preamble on the existing DS implementation}}

My DS runs a specialised version of Debian called Proxmox VE which is an open-source bare-metal hypervisor with a non-commercial licensing tier \cite{proxmox_2020}. Proxmox VE's underlying technology is based on the existing Linux KVM with Qemu hardware virtualiser which is both computationally fast and resource efficient, two prerequisites for this project.

One of the major advantages (and considerations) I can find in using this setup is I/O. Whereas in a standard cluster computer, your bottle-neck is almost always going to come externally (i.e. from the network or associated adaptors) the DS offers a much higher theoretical peak bandwidth between guest OSes. This fact will work in my favour, however there is a chance it could also skew results away from the expected norm, as the bottleneck found in production Beowulf machines will have moved from the network to the component bus.

\textbf{\sffamily{A choice in Guest Operating Systems}}

As my systems will be running atop a hypervisor, I need to make a decision on which flavour of Linux to use for my guest machines. There is a plethora of choice in the headless Linux market, but the main choices ultimately boil down to Debian/Ubuntu, CentOS, RHEL/SLES/openSUSE or Arch/Manjaro \cite{distrowatch_2020}. Of the four candidate OSes, three categories can be identified which might further assist in decision making given the requirements of the project.

First, there is the stable distributions which include both Debian-based OSes and CentOS. These OSes are designed for use on servers and other mission-critical hardware where stability is deemed more important than having up-to-date libraries and drivers. While there would in theory be no issue in using these OSes types for my guest machines, a more up-to-date set of libraries might help me gain additional features from any cluster I attempt to build.

The second of the three OS classifications is enterprise-grade suites which come in the form of RHEL/SLES/openSUSE in this case. Much like Microsoft Windows, enterprise-grade Linux flavours are aimed at businesses and educational institutes and are thus entirely unsuitable for the tasks I will be asking it to perform. For one part, it likely does not have the OpenMPI library in its package repositories which would make doing what I need to do with regards to building a cluster unnecessarily more challenging. For another part, it likely also has packages, libraries and drivers even more out of date than the stable classification of OSes mentioned in the previous paragraph.

Lastly, I come to the third classification which is rolling release distributions. These distributions are typically more feature-bare and unstable than the previous two categories, instead the operator is expected to build a bespoke system using a rich and up-to-date package repository. Arch Linux is the most popular rolling Linux distribution with Manjaro being a forked Arch distribution with additional tools to aid in user-friendliness.

I am choosing to use Manjaro for my project due to its merits as a bleeding-edge distro, where Manjaro Architect shall allow me to design an image with very low memory and CPU overhead and the bare-bones package subset I will need to run and test my cluster. \cite{manjaro_2020}.

\textbf{\sffamily{Measuring the system: A benchmarking software}}

In order to provide metrics on the performance of my system as nodes are scaled and introduced, I will need to benchmark them. While there are benchmarking suites out there such as LINPACK \cite{dongarra_1979} which will do a commendable job in assessing my cluster's performance, I wish to take the challenge upon myself to program a suitable binary for this job and there are two distinct reasons for this.

Firstly, LINPACK is a complex suite of benchmarking softwares which, while tried and tested, are also barely understood by myself and it would take a comprehensive reading far outside the scope of this project for me to come to understand. My logic stands that if I were to write my own benchmarking software, I could both learn how parallel programming is done and also offer deeper insight into what the results mean than if I were to use a prepackaged binary to do the same job.

Secondly, LINPACK takes an extraordinary amount of time and memory to do its job. While this wouldn't necessarily be an issue on a singular test run or on a system where the computational power is exceptionally large, my system fits neither of those descriptors. With my own benchmarking software, I can design in a mechanism to allow for well-scaled performance curves that still offer a picture of the system's overall power availability.

To fulfil this aim, I will need as a basic requirement to maximise CPU time used on all given nodes. A computationally expensive problem must be found and during testing CPU readings on each node must be taken during each run to confirm the process is fulfilling these requirements.

\textbf{\sffamily{A final word: Other software packages required}}

I've discussed in detail the implementation of MPI I will be using in my literature review, as well as the existing software running on my DS that will allow guest OSes to be spawned and communicate with each other. I have also performed a condensed look on the types of OS I could and will use for my project. I will now close this portion of the chapter by discussing the other software packages that will be needed to enable my Beowulf cluster.

Assuming my guest OSes have been installed and are online, I will need a working installation of the GNU Compiler Collection (GCC) \cite{gnu_2020}. GCC is a prerequisite of OpenMPI, which uses a modified version of GCC to compile its own binaries for use with the MPI runtime element.

I will also need a working installation set to run at boot of the OpenSSH Daemon \cite{openbsd_2020}. The SSH protocol appears to be the recommended method for tunnelling between nodes within an MPI cluster due to its lightweight and always-on nature, as well as for its secure cipher suite in Arch/Manjaro OSes.

These packages fundamentally meet the requirements of the project. Additional, optional components may be added on the fly but I cannot currently predict any further relevant packages being required.

\section{My proposed system: In closing \& additional components}
With functional hardware and software, there is little else to write about regarding requirements. Indeed, there may be insufficient words for me to attribute to this section to justify making it a section at all, however I feel it is worth acknowledging as a separate discussion from the topics of hardware and software.

\textbf{\sffamily{Network Hardware: The Physical Switch, Virtual Switch \& NICs}}

Without discussing topologies (as this is a major discussion I plan on having in the next chapter), the network hardware I plan to use consists of three sub-components: the physical switch, the virtual switch and the network interface cards (NICs).

The physical switch is what connects the physical server to my home network. It is a 1 000 BASE-T Ethernet switch with 24-ports allowing for good excitability within this project. As no computational exchanges will be happening over the physical network, Gigabit speeds are sufficient in this project, especially given how my edge speeds are only 300 Megabits per second (or 35 Megabytes per second).

The virtual switch is the software switch which facilitates communications within the hypervisor, both between connected nodes and the outside world. This software switch is already configured on my DS for optimal performance and for network bonding, allowing for four times external throughput when required.

Lastly, the NICs, both physical and virtual. As mentioned, the physical NICs are four bonded 1 000 BASE-T Ethernet devices allowing for 4 Gigabits per second (or 0,5 Gigabytes per second). Additionally, the virtual switch and hypervisor virtual NICs both allow for bandwidths of 100 000 BASE-T or upwards of 100 Gigabits per second (or 12,5 Gigabytes per second) which should remove any bottleneck that could exist between the machines at a network level, instead placing the strain on the system's buses. This will change the dynamic of how the cluster will interact considerably, something I will need to heavily reflect on in my review of the project.
